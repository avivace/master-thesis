\documentclass[a4, oneside, 10pt, nobib]{memoir}

\usepackage{fontspec}
	% Numbers={OldStyle,Proportional},Ligatures=TeX
	\setmainfont[]{Minion Pro}
	\setmonofont[Scale=0.85]{Iosevka}

\begin{document}
\thispagestyle{empty}

\begin{center}

	{\Large Antonio Vivace}
	\\
	\vspace{8mm}
	{\Huge \textbf{Modernising the CERN CMS Trigger Rates Monitoring software}}
	\\
	\vspace{8mm}
	{\huge Master Thesis Summary}
\end{center}

\pagebreak

		\paragraph{CERN, LHC and CMS} High Energy Physics experiments involve large amount of complex systems subject to anomalies and malfunctioning. They generate a lot of high dimensional data that must be monitored and analysed by various specialised teams to validate and deliver certified data for physics analysis. Most of these tasks are still carried out completely by humans, by looking at highly-contextual data and plots in complex workflows. Machine Learning techniques and approaches are being evaluated to aid and possibly automatise these processes.

		CERN, the European Organization for Nuclear Research, operates the Large Hadron Collider, the world's largest and highest-energy particle collider and the largest machine in the world.

		2500 staff members design, construct and operate the research infrastructure, in addition to more than 17500 users and scientists of 110 nationalities, from institutes in more than 70 countries.

		The Compact Moun Solenoid detector in the LHC is a complex system which needs fast and reliable monitoring. Quick feedback on each of the subsystems is essential to spot and solve problems or the data taken might not be interesting for physics analysis. Experts from different systems need to correlate information to investigate underlying problems.

		A centralized monitoring solution must expose real time data, historical information, summaries and reports from a series of heterogeneous sources. The WBM software covered this role since the commissioning of the CMS experiment (2008), evolving and integrating new services into a growing framework during LHC Run 1 (2010-2013) and Run 2 (2015-2018).


		\paragraph{Monitoring the Trigger System}

		A Trigger system is responsible of filtering the large majority of events, spotting the potentially interesting ones, triggering the detector's read-out system to actually record data from the selected collissions.

		One of the major challenges for the Compact Muon Solenoid (CMS) experiment, is the task of reducing event rate from roughly 40 MHz down to a more manageable 1 kHz while keeping as many interesting physics events as possible. This is accomplished through the use of a Level-1 (L1) hardware based trigger as well as a software based High-Level Trigger (HLT). Monitoring and understanding the output rates of the L1 and HLT triggers is of key importance for determining the overall performance of the trigger system and is intimately tied to what type of data is being recorded for physics analyses.

		This crucial phase is also sensible to malfunctions of many of the underlying parts, from sub detectors to the trigger algorithms configurations: monitoring the rates of such filters is essential to spot any anomalous behaviour in the underlying (sub)systems, software and/or hardware configurations, network and detector malfunctions.

		This work concerns RateMon, the software framework providing the monitoring data on this Trigger system: it exposes Trigger Rates data, querying the OMDS database and carrying out \textbf{normalisations} and \textbf{corrections} for a number of different configurations and conditions, allowing consistent comparisons.

		Trigger Rates are presented in the form of \textit{Rates VS PU} \textbf{plots}. It also responsible of \textbf{alerting} the Trigger Shifters staff when recorded rates deviate too much from the \textbf{predicted} values. Those predictions are based on analytical models fitted on data collected in previous runs.

		This software is also used by shifters in the CMS Control Room to monitor Trigger Rates and spot 
		potential problems during the LHC runs.
		
		\paragraph{Upgrading the monitoring framework} 

		During the second Long Shutdown of the LHC (2018-2021) the CMS detector will be upgraded and many CMS sub-systems will drastically change. WBM started to show its age and problems: it unexpectedly and heavily grew with new features, arriving at a point where services were using vastly different technologies and it became harder and harder to mantain and expand. It has been decided to deprecate WBM in favor of a new software framework, called OMS, decoupling the UI from the Aggregation (Data) Layer.

		\paragraph{Overview}

		We proceed to improve and renovate the set of tools included in RateMon, getting them ready to be migrated to the new OMS infrastructure: the tool can be now run and deployed in a reproducible and isolated way. A new CI/CD pipeline is implemented to streamline the development process and deliver software updated directly to the Control Room machines. A new API surface exposes the software analysis, model fitting and plotting capabilities.

		The following goals have been achieved and delivered in production:

		\begin{itemize}

		\item Upgraded the tools to run on a recent and supported version of Python.
		\item Moved the database connection configuration class to an actual configuration file.
		\item Formally defined the environment and the dependencies needed to run the software.
		\item Prepared scripts to package the software with the RPM package format. Prepared a basic systemd service to include in the package, to allow running the software as a system service.
		\item Prepared a Continuos Integration and Deployment pipeline running on the CERN infrastructure.
		\item Upgraded the CI/CD pipeline to use the CMS Cactus auto DevOps tools.
		\item Upgraded the exporting feature of the tools, previously limited to shell output and small PNG image renders.
		\item Enabled the tools to export trigger rates data in a agnostic format, not tied to ROOT.
		\item Enabled the tools to export trigger rates data in ROOT files, allowing ROOT clients to read them.
		\item Updated the documentation adding a a way to consistently prepare P5, LXPLUS and personal machines to run RateMon tools, without being tied to a particular machine cluster at CERN.
		\item Enabled any user to import the RateMon software as a Python module, adding some external-facing and transparent methods to get Trigger Rates as raw data or plots.
		\item Investigated a solution to integrate RateMon into the new CMS Online Monitoring System, replacing the old Web Based Monitoring, now in deprecation.
		\item Since a proper integration was not possible, implemented an API exposing the exporting, processing and plotting functionalities of the RateMon software. This API is formally defined with an OpenAPI 3 Schema and an auto generated Swagger interface is available as documentation to users.
		\item Implemented a reactive web application in VueJS to demonstrate and showcase the API capabilities, plotting Trigger Rates (and their predicted values, based on the fits) in the browser with interactive and scalable plots (powered by JSROOT) instead of static image exports, consuming the new API.
		\item Implemented a Trigger Selection interface on the UI, a feature requested by Trigger shifters and coordinators.
		\item Reported a series of bugs happening in the data processing phase of the scripts, detailing scenarios triggering them, to allow future work on them.
		\item Deployed the implemented stack in production. Set up a separate data storage solution to cache computed data, limiting the bottleneck effect of the CMS database on the API and offering faster responses.

		\end{itemize}


		\paragraph{Building a new dataset to aid automated Anomaly Detection approaches}

		Trying to fit traditional and (untradional) Machine Learning pipelines and techniques in this task is not a noval idea: an elaborated research study has been conducted on Anomaly Detection and Machine Learning approaches to similar problems at CERN, from Variational Auto Encoders solutions (PhD thesis, \cite{}) to improve the analytical models to fit and predict trigger rates to isolation forest and gaussian mixture models to detect anomalies in the LHC injection magnets.

		Similar papers and state of the art studies have been surveyed to frame and formally define our problem.

		Obtaining large, proportionally labeled datasets and the intrinsic difficulty to codify and formalise the expertise in a "consumable" form for traditional ML pipelines are the most critical points of these attemps.

		Therefore, we exploited the acquired knowledge on the software sources to produce a new integrated and labelled dataset.

		Additional work has been done to properly export data from another another important source at CMS, the Run Registry. It exposes the status over time of every part of the detector, including the ones for which the failures are identifiable from the monitoring of the trigger rates, the process currently done by humans by looking at trigger plots and fits.

		These improvements enabled trigger rates time series and detector data to be properly exported and integrated in a new, rich dataset of genuine and labeled anomalous runs, possibily providing a new starting point for novel approaches to automatising this task.


\end{document}