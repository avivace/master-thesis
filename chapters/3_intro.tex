\chapter{Introduction}

\section{Motivations}

\paragraph{Monitoring a a particle detector} The CMS detector at the LHC particle accelerator is a complex system which needs fast and reliable monitoring. Quick feedback on each of the subsystems is needed to spot and solve problems or the data taken might not be useful for physics analysis \cite{chep2016wbm}. Experts from different systems need to correlate information to investigate underlying problems.

A centralized monitoring solution exposes real time data, historical information, summaries and reports from a series of different sources:

\begin{itemize}
	\item Luminosity, Collision Rates.
	\item Global Trigger, \textbf{Trigger Rates}.
	\item LHC (Beam currents, losses, status, collimators, real time clock, event signal).
	\item Magnet.
	\item Sub-system specific information stored in relational databases.
	\item DAQ.
	\item DQM.
	\item Experimental running conditions in database.
	\item Hardware.
	\item Other non event data.
\end{itemize}

Such system is different from the Data Quality Monitoring service, which look at actual \textit{event} data.

The WBM software covered this role since the commissioning of the CMS experiment (2008), evolving and integrating new services into a growing framework during LHC Run 1 (2010-2013) and Run 2 (2015-2018).

\paragraph{Upgrading the monitoring framework} 

During the second Long Shutdown of the LHC (2018-2021) the CMS detector will be upgraded and many CMS sub-systems will drastically change. WBM started to show its age and problems: it unexpectedly and heavily grew with new features, arriving at a point where services were using vastly different technologies and it became harder and harder to maintain and expand \cite{CMSWBMreview}. It has been decided to deprecate \cite{upgradewbmoms} WBM in favour of a new software framework, called OMS, decoupling the UI from the Aggregation (Data) Layer.

\paragraph{Monitoring the Trigger System}

The CMS Trigger System is responsible of filtering the large majority of events, spotting the potentially interesting ones, triggering the detector's read-out system to actually record data from the selected collisions. Monitoring the rates of such filters is essential to spot any anomalous behaviour in the underlying (sub)systems, software and/or hardware configurations, network and detector malfunctions. Without an effective and performant trigger system, physics analysis at CMS would not be possible.

This work concerns one of the sources of monitored data: RateMon \cite{Smith:2293136}, the software providing Trigger Rates data developed by the Field Operation Group. It queries the OMDS database and carries out \textbf{normalisations} and \textbf{corrections} for a number of different configurations and conditions, allowing consistent comparisons.

Trigger Rates are presented in the form of \textit{Rates VS PU} \textbf{plots}. Plots area automatically produced on an hourly basis and uploaded to a web area for inspection.

RateMon is also responsible of \textbf{alerting} the Trigger Shifters staff when recorded rates deviate too much from the \textbf{predicted} values. Those predictions are based on analytical models (called fits) fitted on data collected in previous runs using linear and non-linear regression. These fits are then compared to the instantaneous trigger rate as data are being collected, in order to spot small (unexpected) deviations in rate. 

The software also provides a variety of additional features that are used in offline analysis.

\section{Scope}

These main tasks have been carried out during this experience:

\begin{itemize}

	\item Background study on CMS, the Trigger System and their related monitoring software. Upgrade, maintenance and further development on the Rate Monitoring Software. Prepare the software to be integrated into the new monitoring framework.
	\item Survey and study existing Anomaly Detection approaches, generic and specific to CERN applications, focusing on the ones with comparable characteristics to our problem. Using the improvements on the Rate Monitoring software and the new CMS Run Registry (a new, still in development, tool) build a proper dataset to tackle the the problem of detecting faults on the CMS detector by looking at anomalies at the Trigger Rates and other related data. 

\end{itemize}

\section{Structure}

We will start by introducing CERN in Chapter \ref{background}, the research institute operating the LHC, briefly describing its relevance and its achievements in science, software and computing, besides its major commitment in fundamental research.

To justify the motivations and the unique magnitude of the Large Hadron Collider particle accelerator, some basics notions and concepts in Particle Physics are explained and contextualised. In particular, how statistics plays a vital role in this research field, providing some of the main tools to validate theories. A brief description of LHC and the CMS detector follows, explaining some of the jargon used in this domain, essential to understand the context of our work.

Finally, some background on Anomaly Detection and Machine Learning is given, surveying some applications and recent papers.

Chapter \ref{ratemon} gives a detailed report of the context, the research and development work done on the Rate Monitoring tools, outlining the achieved results and how they improve the user experience and enable new features and possibilities.

In chapter \ref{dataset} we introduce some Anomaly Detection applications to similar challenges at CERN, evaluating their results and possible practical improvements to the current workflows. One of the common problems in such attempts appears to be the inherent difficulty to obtain usable datasets encompassing enough features to encode the phenomena and the contextual knowledge and data considered in the current pipelines.

We exploit the acquired knowledge and the new developed features on the Rate Monitoring software to tackle exactly this issue: we integrate Trigger Rates data with another dataset, potentially providing a new starting point to enable future experimentations on this matter.


\section{Conventions}

Implementation work on existing software was done through Merge Requests to the main codebase, triggering discussions and code reviews. Each described task is accompanied with a corresponding sitography entry.

Code snippets (called "listings", from now on) demonstrating the execution of commands in a shell are noted with the \mcode{\$} character. Preceding the \mcode{\$} character you sometimes notice a string specifying the hostname of the machine (as specified in CERN OpenStack) or the general infrastructure/service in which that commands must run to have effect. E.g. \mcode{lxplus \$ command} shows the command execution on one of the machine from LXPLUS \cite{LXPLUSServiceITDepartment-2020-10-01}, the CERN service offering access to machines running Linux CERN CentOS 7. If there's no specification before \mcode{\$}, the environment is not relevant. With \mcode{P5} we refer to the machines installed in Point 5 of the LHC, in Cessy (France), home to the CMS detector.

The produced source code has been reported here only partially, focusing only on the relevant and meaningful parts. Refer to the git repositories for the complete copies. Some listings also have truncated outputs for the same reasons.