\section{Anomaly Detection}

In data mining, \textit{Anomaly Detection} is a classification problem: the goal is finding patterns in data that do not conform to expected behavior \cite{chandola2009anomaly}. These patterns often denote an underlying different process: an anomaly can be defined an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism \cite{hawkins1980identification}.

This task finds extensive use in many domains, such as fraud detection for credit cards, insurance or health care, intrusion detection for cyber security, military surveillance and fault detection in critical systems.

Anomalies are also referred to as outliers, novelties, deviations, exceptions or noise. \textit{Noise removal} and \textit{noise accomodation} are related problems, dealing with the removal of unwanted objects before performing data analysis (removal) and immunizing a statistical model against anomalous observations (accomodation).

Methods make use of tools and concepts from a number of different fields, such as machine learning, data mining, information theory, spectral theory and statistics.

\subsection{Challenges}

Anomaly detection is an hard and complex problem, mainly due to the following challenging factors:

\begin{enumerate}
	\item Defining the normal and anomalous regions and their boundaries;
	\item The notion of \textit{normality} may evolve with time;
	\item Anomalies may be different for different domains; A small fluctuation might be significant in one domain while being a normal in another one;
	\item Availability of labeled data;
	\item Noisy data and anomalies can be hard to discriminate.
\end{enumerate}

To justify the variety of strategies and techniques and show the breadth of problem domain, we will outline the several factors determining an Anomaly Detection problem.

\subsection{Nature of data}

The input is usually a collection of data instances (record, event, observation), described by one (univariate) or more (multivariate) attributes (feature, variable). Each attribute can be of different types (binary, categorical, continuos).

\subsection{Type of Anomaly}

\begin{enumerate}
	\item Point Anomalies
	\item Contextual Anomalies
	\item Collective Anomalies
\end{enumerate}


\subsection{Data labels}

The information associated with a data instance, describing if the instance is normal or anomalous is called label.


\subsubsection{Supervised}
\subsubsection{Semi-supervised}
\subsubsection{Unsupervised}

\subsubsection{Evaluation}

Here, we introduce some classification performance metrics used to set up a formal procedure to evaluate different approaches.

\begin{table}[]
	\centering
	\begin{tabular}{lllll}
		                                     &          &                                           &          & \\
		\multirow{2}{*}{}                    &          & \multicolumn{2}{l}{\textbf{Ground Truth}} &            \\
		                                     &          & Positive                                  & Negative & \\
		\multirow{2}{*}{\textbf{Prediction}} & Positive & TP                                        & FP       & \\
		                                     & Negative & FN                                        & TN       &
	\end{tabular}
	\caption{Confusion Matrix}
	\label{table:confusion_matrix}
\end{table}

Table \ref{table:confusion_matrix} explains the following quantities:

\begin{itemize}
	\item TP True Positives
	\item FP False Positives
	\item FN False Negatives
	\item TN True Negatives
\end{itemize}

Receiver Operating Characteristics (ROC) diagrams show a classifier performance plotting the False Positive Rates (FPR) against the True Positive Rates (TPR) of the classifier for a different thresholds. The idea is maximise the area under the ROC curve.

Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative while recall is a measure of the ability of the classifier to find all the positive samples. \cite{scikit-learn}


\begin{equation}
	TPR = Recall = r = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
	FPR = \frac{FP}{FP+TN}
\end{equation}

\begin{equation}
	Precision = p = \frac{TP}{TP+FP}
\end{equation}

Another metric is the F-measure, defined as follows:

\begin{equation}
	F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{p} \cdot \mathrm{r}}{(\beta^2 \cdot \mathrm{p}) + \mathrm{r}}
\end{equation}

Where $\beta$ is a positive real chosen such that recall is considered $\beta$ times as important as precision. In the case $\beta = 1$:

\begin{equation}
	F_1 = 2 \cdot \frac{\mathrm{p} \cdot \mathrm{r}}{\mathrm{p} + \mathrm{r}}
\end{equation}



\subsection{Output of Anomaly Detection}

\subsection{Applications}


