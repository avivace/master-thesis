\chapter{Background}
\label{background}
\section{CERN}

The European Organization for Nuclear Research is a research organization that operates the world’s largest and most powerful particle accelerator, the Large Hadron Collider.

2500 staff members design, construct and operate the research infrastructure, in addition to more than 17500 users and scientists of 110 nationalities, from institutes in more than 70 countries.

The LHC collides two beams (\textit{bunches}) of protons at a combined energy of 13 TeV, accelerating them around a 27 km ring of superconducting magnets until they reach the 99,9999990 \% of the speed of light.

Exploiting these collisions is the responsibility of large international scientific \textit{collaborations}. Each experiment, guided by a collaboration, has a detector, a large experimental apparatus placed at a collision point (shown in figure \ref{fig:cern_complex}).

The biggest of these experiments, ATLAS and CMS, are general-purpose detectors, used to investigate the largest range of physics possible. Having two independently designed detectors is essential for cross-confirmation of any new discoveries made.

\begin{figure}
	\centerline{
		\includegraphics[width=0.75\paperwidth]{CCC-2019}}
	\caption{CERN Accelerators complex \cite{Mobs:2684277}}
	\label{fig:cern_complex}
\end{figure}

\begin{figure}
	\centerline{
		\includegraphics[width=0.4\paperwidth]{ip_1601_05235}}
	\caption{Schematic layout of the LHC collision points and beams \cite{Herr:1982430}}
	\label{fig:ip}
\end{figure}

\subsection{Computing}

Besides fundamental research, the Laboratory also plays a vital role in developing and pushing new technologies in various fields, such as materials science and computing.

Starting from early 1980s, CERN pioneered the introduction of Internet technology. The World Wide Web was initiated as "ENQUIRE" by Tim Berners-Lee in 1989 at CERN.

Nowadays, it has become a facility for the development of grid computing, hosting the LHC Computing Grid.

CERN Openlab, a collaboration between CERN and industrial partners (Intel, Oracle, Google, Micron, Siemens, IBM, among others) leads research efforts to develop new knowledge in Information and Communication Technologies through the evaluation and experimentation of innovative tools, such as Quantum Computing and Machine Learning.

\subsection{Open Source, Open Science}

CERN is a primary actor in pushing science forward and donates most of its data, software and hardware designs to the community, releasing them under permissive licenses. It has continuously been a pioneer and first adopter of the Open Science and Open Source philosophy.

Among the most important initiatives in this regard, we mention:

\begin{itemize}
	\item The World Wide Web software was immediatly (1993) licensed as free and open source software. On 30 April 1993 CERN issued a public statement stating that the three components of Web software (the basic line-mode client, the basic server and the library of common code) were put in the Public Domain \cite{LicensingtheWebCERN-2020-10-16}.
	\item The CERN Open Hardware Licence (CERN–OHL) is to hardware what the free and open-source licences are to software. It defines the conditions under which a licensee will be able to use or modify the licensed material. It shares the same principles as free software or open-source software: anyone should be able to see the source – the design documentation in the case of hardware – study it, modify it and share it \cite{HomeWikiProjectsCERNOpenHardwareLicenceOpenHardwareRepository-2020-10-16}.
	\item CERN Open access commitment: the Sponsoring Consortium for Open Access Publishing in Particle Physics is an international collaboration promoted by CERN in the high-energy physics community to convert traditional closed access physics journals to open access \cite{OpensourceforopenscienceCERN-2020-10-16}.
	\item CERN Open data commitment: The CERN Open Data portal allows the LHC experiments to share their data with a double focus: for the scientific community, including researchers outside the CERN experimental teams, as well as citizen scientists, and for the purposes of training and education through specially curated resources \cite{OpensourceforopenscienceCERN-2020-10-16}.
	\item MAlt, a CERN initiative to investigate the migration from commercial software products (Microsoft and others) to open-source solutions, to minimise CERN’s exposure to the risks of unsustainable commercial conditions, vendor lock-ins and to keep the data in own control. CERN is first-hand undergoing these migrations in its huge and complex IT infrastructure. Public research institutions are facing the same dilemma and can make use of CERN experience in those migrations to follow similar paths \cite{MigratingtoopensourcetechnologiesCERN-2020-10-16}.
	\item The CERN Digital Memory project is investigating the creation of an OAIS \cite{oais1} \cite{HomeOAISReferenceModelISO14721-2020-10-16} compliant “Archive as a Service” that enables researchers to conveniently preserve their valuable research in the long term \cite{vanKemenade:2728246}.
\end{itemize}

Furthermore, these are some of tools and technologies directed and funded by CERN (or related European Union grants) that are being released and developed under an Open Source model:

\begin{itemize}
	\item Invenio
	\item Indico
	\item INSPIRE
	\item Zenodo
	\item ROOT
\end{itemize}

\section{Statistics in Physics}

Many problems in physics are described or can be approximated by a small group of probability distributions. We will briefly describe them, focusing on important features exploited by Physics and Particle Physics \cite{leo2012techniques} analyses.

\subsection{Systematic and Random Errors}

Any kind of measurement is subject to two main types of uncertainties:

\begin{enumerate}
	\item \textit{Systematic Errors} are consistent uncertainties in the bias of the data, usually associated with the measurement equipment.
	\item \textit{Random Errors} may arise from instrumental imprecision and from the inherent statistical nature of the observed class of events. Many processes in Physics are governed by the probabilistic laws of quantum mechanics, so that the number of events in a given time period is a random variable. Errors caused by the measurement of random processed are called \textit{statistical errors}. Differing values caused by many small factors not controlled by the experimenter are called \textit{instrumental errors}.
\end{enumerate}

While Systematic Errors are generally harder to handle, uncertainties caused by Random Errors follow known distributions (like a Poisson rate) or are empirically determined. As we'll see in \ref{distributions1}, repeating measurements provides larger samples to estimate these uncertanties and get more accurate data.


\subsection{Binomial}

When a problem involve repeated, independent trials with two possible outcomes, the probability is given by the binomial distribution:

\begin{equation}
	P(r)= \frac{N! }{ r! \left( N-r \right)! } p^r (1-p)^{N-r}
\end{equation}

where $p$ is the probability of success in a single trial.

% TODO: Define r and N, too

Mean:

\begin{equation}
	\mu = \sum_{r}r\:\:\:\:P(r) = Np
\end{equation}

Variance:

\begin{equation}
	\sigma^2=\sum_r(r-\mu)^2
\:\:\:\:\:\:
	P(r) = Np(1-p)
\end{equation}

For many practical calculations \cite{leo2012techniques}, using a Gaussian (\ref{eqn:gaussian}) is a good approximation of a Binomial when $N$ is greater than 30 and $p\geq0.05$ (taking into consideration that we are replacing a discrete distribution by a continuous one). When $p\leq0.05$ and the product $Np$ is finite, we can use a Poisson distribution.

\subsection{Poisson}
\label{distributions1}

When the probability is very small ($p \rightarrow 0$) and the numer of trials approaches infinity ($N \rightarrow \infty$) such that the mean $\mu = N p$ remains finite, the \textit{Poisson} distribution occurs:

% TODO: define r and P(r)

\begin{equation}
	\label{eqn:poisson}
	P\left( r \right) = \frac{{\mu^{ r } e ^{-\mu} }}{{r!}}
\end{equation}

If the mean is expressed as the mean per unit dimension ($\mu = \lambda t$) we can rewrite \ref{eqn:poisson} as:

\begin{equation}
	P\left( x \right) = \frac{(\lambda t) ^r {e^{ - \lambda t}} }{{r!}}
\end{equation}

\begin{figure}
	\centerline{
		\includegraphics[width=0.5\paperwidth]{poisson}}
	\caption{Poisson distribution with various values of $\mu$ \cite{leo2012techniques}}
\end{figure}


As an example, consider the \textit{radioactive decay} phenomena: a radioactive source such as $^{137}$Cs has a half-life of 27 years. The probability for a single nucleus to decay (per time unit) is $8.2 \times 10^{-10}s^{-1}$. However, even a \SI{1}{\micro\gram} contains $10^{15}$ nuclei. Since each nucleus acts as a \textit{trial}, the mean number of decay events will be $\mu = N p = 8.2 \times 10^5$, satisfying the limiting conditions. The probability of observing an event is given by \ref{eqn:poisson}.

The Poisson distribution exhibits two interesting features:

\begin{enumerate}
	\item Only mean appears, so the knowledge of $N$ and $p$ is not always required. E.g. this happens in experiments involving particle reactions where the mean counting rate is known, rather than the number of particles in the beam.

	\item This distribution only depends on one parameter: $\mu$. We can arbitrarily increase this value by repeating the experiment for higher values of $N$.
\end{enumerate}

\subsection{Gaussian}
\label{eqn:gaussian}

The \textit{Gaussian} (or \textit{Normal}) is a continuos, symmetric distribution. We define the density as the function whose value at any given sample in the sample space (the set of possible values for the variable) can be interpreted as the relative likelihood that the value of the random variable would assume that sample \cite{Grinstead1997IntroductionTP}. In this case:

\begin{equation}
	P(x) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}
\end{equation}

Where $\mu$ is the mean and $\sigma ^2$ corresponds to the variance. The case where $\mu = 0$ and $\sigma = 1$ is called the \textit{standard} normal distribution.

\begin{equation}
	y = \frac{e^{ - \frac{{x^2 }}{2}}}{{\sqrt {2\pi } }}
\end{equation}

Any Gaussian distribution can be transformed to this reduced form trivially:

\begin{equation}
	z = \frac{x-\mu}{\sigma}.
\end{equation}

\begin{figure}
	\centerline{
		\includegraphics[width=0.5\paperwidth]{gaussian}}
	\caption{Gaussian distribution with various values of $\sigma$ \cite{leo2012techniques}}
\end{figure}

\section{Particle Physics}

Particle physics researches the nature of the fundamental parts of matter and how they interact. According to the Standard Model \cite{Quang:1998yw}, the matter is made of \textit{fermions}, classified into leptons (electrons, muons, taus and neutrinos) and quarks (up, down, charm, strange, top, bottom). Each particle have an associated antiparticle of the same mass but opposite quantum numbers.

Interactions between fermions are mediated by bosons:

\begin{enumerate}
	\item Strong interaction, mediated by gluons.
	\item Weak interaction, mediated by Z and W bosons.
	\item Electromagnetic interaction, mediated by photons.
\end{enumerate}

Gravitational force is not taken into account. Quarks interact with all three forces, leptons are not sensitive to the strong force and neutrinos are subject only to the weak force.

Experiments are the tool to validate the current theories: the Standard Model anticipated the existence of previously undiscovered particles, such as the \textit{Higgs} Boson, observed at CERN in 2012. This particle is responsible of giving mass to the otherwise massless particles in the SM, conciliating the theory with other experimental findings (Brout-Englert-Higgs mechanism \cite{PhysRevLett.13.321, PhysRevLett.13.508}).

Current High Energy Physics research includes:
\begin{enumerate}
	\item Matter-antimatter asymmetry of the universe \cite{Bernreuther:2002uj};
	\item The not-yet discovered dark matter, which is supposed to give galaxies the extra gravity needed to explain many observed features \cite{bertone2005particle};
	\item Explanation of gravitational physics in terms of quantum mechanics (e.g. \cite{Rovelli:2011eq});
\end{enumerate}

\section{Accelerators and the Large Hadron Collider}

A particle accelerator is a machine in which beams of charged particles are accelerated to high energies. Electric fields are used to accelerate the particles while magnets steer and focus them. Beams can be made to collide with a static target or with each other \cite{CERNGlossary}.

There different are types of accelerators:

\begin{itemize}

\item A collider is a special type of circular accelerator where beams travelling in opposite directions are accelerated and made to interact at designated collision points.

\item A linear accelerator (or linac) is often used as the first stage in an accelerator chain.

\item A synchrotron is an accelerator in which the magnetic field bending the orbits of the particles increases with the energy of the particles. This makes the particles move in a circular path.

\end{itemize}

LHC is the highest-energy particle \textit{collider} in the world. It reached 6.5 TeV per beam (13 TeV total collision energy). At four crossing points, seven detectors are positioned (CMS is one of the them, positioned at Point 5), each designed for certain kinds of research. LHC collides proton beams, but it can also use beams of heavy ions: \textit{lead–lead} collisions and \textit{proton–lead } collisions are typically done for one month per year.

\subsection{Run, Fill}

For the LHC, a "run" is a period between "long shutdowns". Run 1 was 2010 to early 2013, Run 2 is 2015 to 2018, etc. There are thousands of LHC fills in a LHC run.

For the experiments, a "run" is a discrete interval of data-taking. Usually run changes occur when something about the detector itself changes, such as trigger configuration or a subdetector is enabled/disabled. There may even be a timer that changes run after a prescribed period (certainly this is the case at LHCb). There can be many runs per fill, or runs can occur independently of fills, particularly if there's no beam or the LHC is in "machine development".

\subsection{Cross Section}

\begin{figure}

	\centerline{
		\includegraphics[width=0.5\paperwidth]{cross_sections}}
	\caption{Standard  model  cross  sections  at  the  Tevatron  and  LHC  colliders\cite{Bechtel:2009zza}}
	\label{fig:cross_sections}
\end{figure}

Cross section ($\sigma$) is one of the most important quantity in particle physics: it measures the probability that a specific process occurs in a collision of two particles. It is expressed in terms of the transverse area that the particle must hit to for that process to occur and it's measured in \textit{Barn}: $1\, b = 10^{-24} \,cm^2$.

E.g. the \textit{Rutherford cross-section} represents the probability of an alpha-particle to be deflected by a given angle during a collision with an atomic nucleus ($2.7 \times 10^{-14} \,m^2$, 0.27 $b$).

Figure \ref{fig:cross_sections} shows $\sigma$ values for different processes searched by accelerators like LHC. Notice how the value increments with the collision energy $\sqrt s$ (13 TeV during LHC Run 2).

\subsection{Luminosity}

Luminosity is a measure of how many particles pass through a given area in a certain amount of time. The higher the luminosity delivered by the LHC, the larger the number of collision events happening at each experiment. Hence, more luminosity means more precise results and an increased possibility to observe rare processes \cite{CERNGlossary}.

More specifically it is a measurement of the number of collisions that can potentially be produced in a point of collision per $cm^2$ and per second. It can be calculated in this way:

\begin{equation}
	L \simeq  \frac{N^2}{t S_{\text{eff}}}
\end{equation}

Where $N$ is the number of protons in each bunch, $t$ is time between bunches, $S_{\text{eff}} = 4 \pi \sigma_c^2$ is the section effective of collision.

In the LHC, $\sigma _c= 16\times 10^{-4}\, cm$, $N = 1.15\times10^{11}$, $t = 25 \times 10^{-9}\, s$, leading to a design luminosity of $10^{34}\, cm^{-2}s^{-1}$.

Pile up can be defined as thhe total number of interactions per bunch crossing \cite{CMSDataAnalysisSchool2529August2015OverviewIndico-2020-10-16}.

\begin{figure}
	\centerline{
		\includegraphics[width=0.35\paperwidth]{lhcpu1}}
	\caption{Bunch crossing rate and Pile Up values for LHC \cite{CMSDataAnalysisSchool2529August2015OverviewIndico-2020-10-16}}
\end{figure}

\begin{figure}
	\centerline{
		\includegraphics[width=0.5\paperwidth]{lhcpu2}}
	\caption{LHC performance in 2010, 2011 and 2012 runs \cite{CMSDataAnalysisSchool2529August2015OverviewIndico-2020-10-16}}
\end{figure}


\subsection{Delivered Luminosity}

The actual luminosity value reached in the points of collision and recorded by the detectors is called Delivered Luminosity.

A number of factors influence this value:

\begin{enumerate}
	\item Adjustments of the LHC beam-optics (emittance scan);
	\item Prescale settings;
	\item Lumi-levelling;
	\item Physiological decrease, due to the fact that protons get consumed by collisions.
\end{enumerate}

Figure \ref{fig:lum_evolution} shows the evolution of the delivered luminosity to the ATLAS, CMS and LHCb experiments.


\begin{figure}
	\centerline{
		\includegraphics[width=0.5\paperwidth]{inst-lumi}}
	\caption{Example for the luminosity evolution of the ATLAS, CMS and LHCb experiments in a typical fill of the 2018 run. The luminosity of LHCb is levelled by beam separation. The upward steps of the ATLAS and CMS luminosities in the second half of the fill are due to $\beta\text{*}$ levelling
		\cite{Wenninger:2018cgs}}
	\label{fig:lum_evolution}
\end{figure}


\label{int_lum_def}
\subsection{Integrated Luminosity}

The integral of the delivered luminosity over time is called integrated luminosity. It is a measurement of the collected data size, and it is an important value to characterize the performance of an accelerator.

\begin{equation}
	L = \int Ldt
\end{equation}

It's expressed in inverse of cross section, usually in $\text{femtobarns}^{-1}$ ("Femto" indicates a factor of $10^{-15}$ so $\text{1 femtobarn} = 1 \text{fb} = 10^{-15} \text{b} = 10^{-39} cm^2 $).

One inverse femtobarn is equal to approximately 100 trillion proton-proton collisions.

\begin{figure}
	\centerline{
		\includegraphics[width=0.5\paperwidth]{integrated_lumi_atlas}}
	\caption{Integrated luminosity at ATLAS experiment \cite{Bruce:2016iew}}
\end{figure}

\textit{Lumi Section} (LS) \label{ls_def} is a unit accounting for Integrated Luminosity. It's defined as a the sub-section of a run during which time the instantaneous Luminosity is unchanging (\~{}23 seconds).

\subsection{Standard Deviation and Significance}

Searches in Particle Physics are done looking for an excess above background: a discovery of a particle involves knowing the amount of background events to except and being confident that the excess is not the result of a statistical fluctuation of this background.

The scientific process requires quantification of the probability that excesses are genuine: this is why the "number of $\sigma$" is crucial in validating discoveries: it represents the width of the Gaussian distribution, as we explained in \ref{eqn:gaussian}. The greater number of $\sigma$, the minor will be the probability that the observed excess can be explained by a background fluctuation.
At $3\sigma$ we talk about an evidence, while at $5\sigma$ we are in front of a discovery.

\begin{figure}
	\centerline{
		\includegraphics[width=0.4\paperwidth]{Standard_deviation_diagram}}
	\caption{Normal distribution curve that illustrates standard deviations. Each band has 1 standard deviation, and the labels indicate the approximate proportion of area \cite{FileStandarddeviationdiagramsvgWikimediaCommons-2020-10-07}}
\end{figure}

\begin{figure}
	\centerline{
		\includegraphics[width=0.4\paperwidth]{2012Higgsplot}}
	\caption{The invariant mass from pairs of photons selected in the Higgs to $\gamma\gamma$ analysis at ATLAS, as shown at the seminar at CERN on 4 July 2012. The excess of events over the background prediction around 125 GeV is consistent with predictions for the Standard Model Higgs boson.\cite{Collaboration:2627611}}
\end{figure}

\begin{figure}
	\centerline{
		\includegraphics[width=0.4\paperwidth]{2012Higgsplot_CMS}}
	\caption{Di-photon ($\gamma\gamma$) invariant mass distribution for the CMS data of 2011 and 2012 (black points with error bars). The data are weighted by the signal to background ratio for each sub-category of events. The solid red line shows the fit result for signal plus background; the dashed red line shows only the background. \cite{Collaboration:1459463}}
\end{figure}


Usually, the background follows a pattern, for which it's possible to fit a function (in this case a fourth order polynomial).

\section{Compact Muon Solenoid}

The Compact Muon Solenoid (CMS) experiment is a general purpose particle detector, designed to observe all interactions in a collision. It's also a \textit{hermetic} detector: it blocks particles from escaping the detector undetected. Since the decay of particles can produce new particles not interacting with any part of the detector, this design allows the measurement of imbalance in momentum and energy, and these non-interacting phenomena can be inferred.

\begin{figure}
	\centerline{
		\includegraphics[width=0.75\paperwidth]{slice_white_colour_french_291016}}
	\caption{A transversal slice through the CMS detector, demonstrating the various sections of the detector
		and their designed functions. \cite{Barney:2628641}}
	\label{fig:cms2}
\end{figure}

\paragraph{Silicon Tracker}

This part, the closest to the center of collision, is able to measure location, magnetic field and momentum of the particles. It can detect the decay of very short-lived particles, such as beauty quarks.

It needs to be as little as obstrusive as possibile, and it's built following the microstrip design: a large number of identical semiconductor strips laid out along one axis of a two-dimensional structure. The geometrical layout of the components allows to accurately reconstruct the track of an incoming particle of ionizing radiation.

\paragraph{Electromagnetic Calorimeter} (ECAL) is composed by a set of 75000 lead lungstate crystals, stopping the passing electrons and photons. When this happens, these crystals produce light as electromagnetic photon showers, measured by photodetectors.


\paragraph{Hadronic Calorimeter} (HCAL) detects hadrons and gluons with several layers of dense absoring materials and \textit{scintillators} where light pulses are produced when particles flows through.

\paragraph{Superconducting Solenoid} The largest superconducting magnet ever built. It's a huge magnet made of coils of wire that produce a uniform magnetic field when electricity flows through them. The CMS magnet is “superconducting”, allowing electricity to flow without resistance and generating a powerful magnetic field of 4 T, allowing precise momentum readings by analyzing the arcs of charged particles.

\paragraph{Muon Chambers}

This section is composed of three components: Resistive Plate Chambers (RPC), Drift Tubes (DT), Cathode Strip Chambers (CSC). It relies on the strong magnetic field (\~{} 2 T, provided by the return flux outside the solenoid) to measure the curvature of the only particles that managed to reach this section: muons and neutrinos.


\begin{figure}
	\centerline{
		\includegraphics[width=0.8\paperwidth]{cms}}
	\caption{Cutaway diagram of the CMS detector \cite{Sakuma_2014}}
	\label{fig:cms}
\end{figure}